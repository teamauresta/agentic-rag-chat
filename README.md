<p align="center">
  <h1 align="center">ğŸ¤– Agentic RAG Chat</h1>
  <p align="center"><strong>Self-hosted AI chat platform with RAG, guardrails, and streaming</strong></p>
  <p align="center">
    <img src="https://github.com/teamauresta/agentic-rag-chat/actions/workflows/ci.yml/badge.svg" alt="CI">
    <img src="https://img.shields.io/badge/python-3.12-blue?logo=python" alt="Python">
    <img src="https://img.shields.io/badge/FastAPI-0.115-009688?logo=fastapi" alt="FastAPI">
    <img src="https://img.shields.io/badge/pgvector-PostgreSQL-336791?logo=postgresql" alt="pgvector">
    <img src="https://img.shields.io/badge/Redis-7-DC382D?logo=redis" alt="Redis">
    <img src="https://img.shields.io/badge/license-MIT-green" alt="MIT License">
  </p>
</p>

---

Deploy your own AI assistant in minutes. Connect any OpenAI-compatible LLM (vLLM, Ollama, OpenAI, Together, etc.), upload documents for RAG, and get a production-ready chat API with guardrails, session management, and a beautiful widget.

## âœ¨ Features

- ğŸ”Œ **Any LLM Backend** â€” Works with vLLM, Ollama, OpenAI, Together, or any OpenAI-compatible API
- ğŸ“„ **RAG Pipeline** â€” Upload PDFs, DOCX, CSV, TXT, MD â†’ auto-chunked, embedded, and searchable via pgvector
- ğŸ›¡ï¸ **3-Layer Guardrails** â€” Input filtering, streaming sanitisation, output validation
- âš¡ **SSE Streaming** â€” Real-time token streaming to the client
- ğŸ’¬ **Session Management** â€” Redis-backed conversation history with automatic summarisation
- ğŸ”‘ **API Key Auth** â€” Simple bearer token authentication
- ğŸš¦ **Rate Limiting** â€” Per-IP and per-session rate limits
- ğŸ“Š **Token Management** â€” Automatic history trimming with LLM-powered summarisation
- ğŸ“ **File Upload API** â€” Upload and index documents via REST API
- ğŸ¨ **Chat Widget** â€” Beautiful, configurable HTML widget (dark mode, markdown, file upload)
- ğŸ³ **Docker Ready** â€” `docker compose up` and you're running
- ğŸ”’ **Self-Hosted** â€” Everything runs on your infrastructure. No data leaves your network.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Chat Widget (HTML)                     â”‚
â”‚              or any HTTP client / frontend                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTPS / SSE
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Agentic RAG Chat API                     â”‚
â”‚                     (FastAPI + Python)                    â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   Auth   â”‚  â”‚ Guardrailsâ”‚  â”‚  Tokens  â”‚  â”‚  Rate   â”‚â”‚
â”‚  â”‚  (API    â”‚  â”‚ (3-layer) â”‚  â”‚ (tiktokenâ”‚  â”‚ Limiter â”‚â”‚
â”‚  â”‚   keys)  â”‚  â”‚           â”‚  â”‚  + trim) â”‚  â”‚         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   RAG Engine     â”‚  â”‚   Session Manager (Redis)    â”‚  â”‚
â”‚  â”‚ (embed + search) â”‚  â”‚   (history + rate limits)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                           â”‚
            â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL +     â”‚        â”‚     Redis        â”‚
â”‚  pgvector         â”‚        â”‚                  â”‚
â”‚  (embeddings)     â”‚        â”‚  (sessions)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ SSE Stream
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Any OpenAI-Compatible LLM          â”‚
â”‚  (vLLM, Ollama, OpenAI, Together...)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. Clone and configure

```bash
git clone https://github.com/sotastack/agent.git
cd agent
cp .env.example .env
# Edit .env with your LLM endpoint and API key
```

### 2. Start services

```bash
docker compose up -d
```

### 3. Ingest sample documents and chat

```bash
# Ingest the sample docs
docker compose exec agent python ingest.py --path docs/ --source "Sample Docs"

# Test the API
curl http://localhost:8083/api/v1/health

# Open the widget
open widget/index.html
```

That's it. You're running a self-hosted AI assistant with RAG.

## ğŸ“– Configuration

All configuration is via environment variables. See [`.env.example`](.env.example) for the full list.

| Variable | Default | Description |
|----------|---------|-------------|
| `LLM_URL` | `http://localhost:8000/v1` | OpenAI-compatible API endpoint |
| `LLM_API_KEY` | - | API key for your LLM backend |
| `LLM_MODEL` | `default` | Model name to use |
| `CLIENT_API_KEYS` | - | Comma-separated API keys for client auth |
| `REDIS_URL` | `redis://localhost:6379/0` | Redis connection URL |
| `RAG_DB_HOST` | `localhost` | PostgreSQL host |
| `RAG_ENABLED` | `true` | Enable/disable RAG |
| `RAG_TOP_K` | `5` | Number of RAG results to inject |
| `RAG_MIN_SIMILARITY` | `0.3` | Minimum cosine similarity threshold |
| `MAX_TOKENS_CONTEXT` | `28000` | Max tokens in context window |
| `RATE_LIMIT_PER_MIN` | `20` | Per-IP rate limit |

## ğŸ”Œ LLM Backend Examples

**vLLM (local GPU):**
```env
LLM_URL=http://localhost:8000/v1
LLM_API_KEY=token-abc123
LLM_MODEL=meta-llama/Llama-3-8B-Instruct
```

**Ollama:**
```env
LLM_URL=http://localhost:11434/v1
LLM_API_KEY=ollama
LLM_MODEL=llama3
```

**OpenAI:**
```env
LLM_URL=https://api.openai.com/v1
LLM_API_KEY=sk-...
LLM_MODEL=gpt-4o
```

## ğŸ“¡ API Reference

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/v1/health` | Health check |
| `POST` | `/api/v1/chat` | Send a message (SSE streaming response) |
| `POST` | `/api/v1/upload` | Upload a document for RAG indexing |
| `GET` | `/api/v1/files` | List indexed documents |
| `GET` | `/api/v1/session/{id}` | Get session info |
| `DELETE` | `/api/v1/session/{id}` | Delete a session |

### Chat Request

```bash
curl -X POST http://localhost:8083/api/v1/chat \
  -H "Authorization: Bearer your-api-key" \
  -H "Content-Type: application/json" \
  -d '{"message": "What is in the knowledge base?"}' \
  --no-buffer
```

### Upload a Document

```bash
curl -X POST http://localhost:8083/api/v1/upload \
  -H "Authorization: Bearer your-api-key" \
  -F "file=@document.pdf" \
  -F "source=Product Manual"
```

## ğŸ›¡ï¸ Guardrails

Agentic RAG Chat includes three layers of protection:

1. **Input Guardrails** â€” Blocks prompt injection, jailbreak attempts, and model probing
2. **Streaming Sanitisation** â€” Strips unwanted characters (e.g., CJK from English-only models) in real-time
3. **Output Validation** â€” Checks completed responses for system prompt leaks

Customise blocked patterns in `guardrails.py`.

## ğŸ¨ Widget

The included chat widget (`widget/index.html`) is a single HTML file with zero dependencies. Configure it via URL parameters:

```
widget/index.html?api=http://localhost:8083/api/v1&key=your-key&title=My+Assistant
```

| Param | Description |
|-------|-------------|
| `api` | Agent API base URL |
| `key` | API key for authentication |
| `title` | Custom title in the header |
| `subtitle` | Custom subtitle |

## ğŸ› ï¸ Development

```bash
# Install dependencies
pip install -r requirements.txt

# Run in dev mode
make dev

# Ingest sample documents
make ingest

# Health check
make test
```

## ğŸ“ Customisation

- **System Prompt**: Edit `prompts/default.txt` or add client-specific prompts as `prompts/{client}.txt`
- **Guardrails**: Modify `guardrails.py` to add/remove blocked patterns
- **RAG Settings**: Adjust `RAG_TOP_K`, `RAG_MIN_SIMILARITY` in `.env`
- **Widget**: The widget is a single HTML file â€” fork and customise freely

## ğŸ“¦ Tech Stack

- **FastAPI** â€” async Python web framework
- **httpx** â€” async HTTP client for LLM streaming
- **Redis** â€” session storage and rate limiting
- **PostgreSQL + pgvector** â€” vector similarity search for RAG
- **sentence-transformers** â€” CPU-based embedding (all-MiniLM-L6-v2)
- **tiktoken** â€” token counting for context management

## ğŸ“„ License

MIT â€” see [LICENSE](LICENSE).

## ğŸ”— Links

- **Website**: [sotastack.com.au](https://sotastack.com.au)
- **Issues**: [GitHub Issues](https://github.com/sotastack/agent/issues)

---

<p align="center">Built by <a href="https://sotastack.com.au">SOTAStack</a> Â· Melbourne, Australia ğŸ‡¦ğŸ‡º</p>
